# üöÄ Airflow + Kafka + Telegram Bot + OpenAI Integration (WORK IN PROGRESS)

This project demonstrates a **modular pipeline** that connects a Telegram bot with Kafka, Airflow, OpenAI and SQLite.  
It allows users to send messages via Telegram, process them with OpenAI, and store numeric responses in a database for downstream workflows.

---

## üì¶ Components

- **Apache Airflow**  
  Orchestrates tasks and manages DAGs. Runs in Docker.

- **Kafka + Zookeeper**  
  Provides a message broker for communication between the Telegram bot and Airflow consumer.

- **Telegram Bot**  
  Non-numeric messages ‚Üí processed by OpenAI (LLM prompt).  
  Numeric messages ‚Üí pushed directly into Kafka.

- **OpenAI API**  
  Generates contextual numeric questions when users send free text.

- **SQLite**  
  Stores consumed Kafka messages for persistence and analysis.

---

## üóÇ Project Structure
```
‚îú‚îÄ‚îÄ docker-compose.yaml # Airflow + Kafka + Redis + Postgres + Bot services  
‚îú‚îÄ‚îÄ bot_app/  
‚îÇ ‚îú‚îÄ‚îÄ bot_producer_llm.py # Telegram bot producer logic  
‚îÇ ‚îî‚îÄ‚îÄ config.py # Secrets (OPENAI_KEY, TELEGRAM_KEY)  
‚îú‚îÄ‚îÄ dags/  
‚îÇ ‚îî‚îÄ‚îÄ kafka_consumer_dag.py # Airflow DAG consuming Kafka ‚Üí SQLite  
‚îú‚îÄ‚îÄ requirements.txt # Python dependencies  
‚îî‚îÄ‚îÄ Dockerfile # Custom Airflow image with bot + requirements
```

---

## ‚öôÔ∏è Setup

### 1. Environment Variables
Create a `.env` file in the project root:

```env
OPENAI_TOKEN=your_openai_api_key
BOT_TOKEN=your_telegram_bot_token
```
### 2. Build & Start Services

```bash
docker-compose build #creates images
docker-compose up #creates containers
```

This will start:

-   Airflow (scheduler, workers, API server, etc.)
    
-   Kafka + Zookeeper
    
-   Telegram bot producer
    

## üìñ Usage

1.  **Start the Telegram bot**
    
    -   Send a message to your bot.
        
    -   If it‚Äôs **text**, OpenAI reformulates it into a numeric question.
        
    -   If it‚Äôs a **number**, it‚Äôs sent directly to Kafka.
        
2.  **Kafka ‚Üí SQLite via Airflow DAG**
    
    -   Airflow DAG (`kafka_consumer_dag`) runs every 5 minutes.
        
    -   Consumes messages from Kafka topic `telegram_topic`.
        
    -   Stores them in `telegram.db` under table `kafka_messages`.
        
3.  **Inspect Stored Data**
    

```bash
sqlite3 telegram.db
sqlite> SELECT * FROM kafka_messages;

```


## üß© Example Flow

-   User sends:
    
    ```Code
    I like to travel!
    ```
    
-   Bot replies (via OpenAI):
    
    ```Code
    How many countries have you visited?
    ```
    
-   User sends:
      
    ```Code
    13
    ```
    
-   Bot acknowledges and pushes `13` ‚Üí Kafka ‚Üí Airflow DAG ‚Üí SQLite.
    

## üõ† Development Notes

-   **Airflow Image**: Extended from `apache/airflow:2.9.0` with extra requirements. Dockerfile was built on top of the original Airflow Dockerfile.
    
-   **docker-compose.yaml** was adjusted to start from an own image and in order to launch Zookeeper, a Kafka broker and topic, as well as the bot.
    
-   **Secrets**: Managed via `.env` file and `config.py`.
    

## ‚ö†Ô∏è Warnings

-   This setup is for **local development only**.
    
-   Do **not** use in production without securing secrets, scaling Kafka, and hardening Airflow.
    
